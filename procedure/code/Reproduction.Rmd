---
title: "Reproduction"
author: "Sarah Bardin and Josh Gilman"
date: "3/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, results='hide', warning = F, message = F)

set.seed(1)
rm(list=ls())

library(tidyverse)
library(spdep)
library(maptools)
library(sp)
library(rgdal)
library(DCluster)
library(readr)
library(readxl)
library(tidycensus)
library(sf)
library(here)
library(spatialreg)
library(maps)
library(patchwork)
library(ggpubr)
library(fdrtool)

output_maps <- here("results", "maps")

```





# Data Preparation
## Create Working File
In order to reproduce Saffary et al.'s paper *Spatial Dynamics of COVID-19 in US Counties*, we need to first create a working data set. To do this, we've downloaded the data provided by Saffary et al. (data_22.05.2020.xlsx) as well as a Census county-level boundary file. We merge the two files together and restrict to counties with non-missing data. This achieves a sample size of 3,142 counties, which is the same sample size reported by Saffery et al. for their analysis.

```{r read_in}
##-----------------------##
#------ READ IN DATA -----#
##-----------------------##
#---County Health Rankings Data---#
chr.pcp <- read_xlsx((here("data", "raw", "public", "2020 County Health Rankings Data - v2.xlsx")), sheet = "Ranked Measure Data", range =("A2:DF3195"))

chr.pcp <- chr.pcp %>%
              transmute(GEOID = FIPS,
                        PCP = `Primary Care Physicians Rate`)
head(chr.pcp)

#---Saffery Data---#
orig.data <- read_xlsx(here("data", "raw", "public", "data_22.05.2020.xlsx"))

#-Clean up variable names-#
names(orig.data) <- substr(names(orig.data),1,regexpr(",",names(orig.data)))
names(orig.data) <- substr(names(orig.data),1,nchar(names(orig.data))-1)
str(orig.data)

#---Boundary Data---#
boundaries <- get_estimates(geography = "county", 
                     product = "population", 
                     year = 2018,                  ## Saffary used 2018 boundary data
                     geometry = TRUE)

boundaries <- boundaries %>% 
                  filter(variable == "POP") 

str(boundaries)
str(orig.data)
str(chr.pcp)
##-----------------------##
#------ MERGE DATA  -----#
##-----------------------##
int.data <- left_join(orig.data, chr.pcp, by = "GEOID")
int.data <- int.data %>%
                dplyr::select(GEOID,
                              ICUBEDS, 
                              PCP,
                              DIABETS, 
                              OBESITY,
                              BLACK, 
                              HISPANC, 
                              WHITE, 
                              UNINSRD, 
                              VACCNTN,
                              CASS100, 
                              DEATH100,
                              CASES,
                              DEATHS,
                              STATENM) %>% 
                dplyr::mutate(PCP_impute_zero = ifelse(is.na(PCP), 0, PCP)) %>% # impute all NA values with 0
                dplyr::mutate(PCP_impute_mean = ifelse(is.na(PCP), 54.5, PCP)) # impute all NA values with mean

str(int.data)
ds <- inner_join(boundaries, int.data, by = "GEOID") 
ds <- st_transform(ds, 5070)
ds <- ds %>% filter(!is.na(STATENM))  ## Results in 3,142 obs which matches article

summary(ds$PCP_impute_zero)
summary(ds$PCP_impute_mean)

##-----------------------##
#------ STATES DATA  -----#
##-----------------------##
states <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) # for highlighting state boundaries as
```



## Reproduce Global Moran's I 
After creating a working data file, we attempt to reproduce the global Moran's I statistics for the cases and deaths outcomes. We must use a the zero.policy = TRUE option in the weights construction, as there are counties with no neighbors in the data set. In part, this is due to the fact that the data set with 3,142 counties includes counties in Hawaii and Alaska. Because Saffery et al. indicated that Hawaii and Alaska were omitted from the analysis, we drop these counties, resulting in a data file with 3,108 counties. We again attempt to reproduce the global Moran's I statistics and again we find that there are counties without neighbors; an investigation into the noncontiguous counties identifed that Nantucket, MA is an island county. Despite needing to allow for counties with no neighbors, we are able to achieve similar I statisitics for both outcomes (to the third decimal place). **NOTE:** With rounding, we'd get an I statistic that is 0.001 higher than reported in the publication.

```{r global, results= T}
##------------------------------------##
#------ PERFORM GLOBAL MORAN'S I  -----#
##------------------------------------##
# Create spatial weights matrix with queen adjacency and binary connectivity
QN <- poly2nb(ds, queen = TRUE)
QN1.lw <- nb2listw(QN, style = "B", zero.policy = TRUE) ## there are empty neighbor sets
                                                        ## so used zero.policy option
moran.test(as.numeric(ds$DEATH100), QN1.lw, zero.policy = TRUE)

##------------------------------------##
#----- TEST OUT DROPPING AK AND HI ----#
##------------------------------------##
ds2 <- ds %>% filter(substr(GEOID,1,2) != "02" & substr(GEOID,1,2) != "15") 
ds2 <- ds2[-c(329,1168,1816),] ##-- EVEN IF WE REMOVE THE 3 COUNTIES WITH NO NEIGHBORS I STATISTIC DOESN'T CHANGE

#--Define Weighting Scheme--#
nb <- poly2nb(ds2, queen = TRUE)
lw <- nb2listw(nb, style = "B", zero.policy = TRUE) ## there are still empty neighbor sets
                                                              ## so used zero.policy option
W  <- as(lw, "symmetricMatrix")
W  <- as.matrix(W/rowSums(W))
W[which(is.na(W))] <- 0

##------------------------------------##
#-------- Filter Matrix for PCP -------#
##------------------------------------##

ds3 <- ds2 %>% 
  filter(!is.na(PCP))  # 143 NA values; 2962 rows after filter.

#--Define Weighting Scheme--#
nb2 <- poly2nb(ds3, queen = TRUE)
lw2 <- nb2listw(nb2, style = "B", zero.policy = TRUE) ## there are still empty neighbor sets
                                                              ## so used zero.policy option
W2 <- as(lw2, "symmetricMatrix")
W2 <- as.matrix(W2/rowSums(W2))
W2[which(is.na(W2))] <- 0

#--DEATH RATE--#
moran.test(as.numeric(ds2$DEATH100), lw, zero.policy = TRUE)

#--CASE RATE--#
moran.test(as.numeric(ds2$CASS100), lw, zero.policy = TRUE)

```

After reproducing the global moran's I for the case and death rates, respectively, we also attempted to reproduce the local univariate moran's I for the case and death rates. We produced maps to mimic those in the original publication to assist in our visual inspection of the results. There are some discrepancies between these maps and those in Figures 1.c and 1.d. of the publication.

```{r local, results= T}
#Create Local Moran's I Plot
theme_set(theme_void())

# set breaks to critical z values
breaks <- c(-100, -2.58, -1.96, -1.65, 1.65, 1.96, 2.58, 100)

#------------------#
#--LISA for Cases--#
#------------------#

ds2$localI_case <- localmoran(as.numeric(ds2$CASS100), lw)[,4]

# findInterval() assigns ranks to the zvalues based on which bin the z values would fall into where the bins are broken up by the "breaks" variable created above
ds2$localI_case_sigbreaks <- findInterval(ds2$localI_case, breaks, all.inside = TRUE)

# Identify high and low clusters using the interaction between the weights matrix and the standardized cases
cases_z <- scale(ds2$CASS100)[,1]
  patterns <- as.character( interaction(cases_z > 0, W%*%cases_z > 0) )
  patterns <- patterns %>%
      str_replace_all("TRUE","High") %>%
      str_replace_all("FALSE","Low")

  patterns[ds2$localI_case_sigbreaks == 4] <- "Not significant"
  ds2$patterns <- patterns

  # Rename LISA clusters
  ds2$patterns2 <- factor(ds2$patterns,
                                     levels=c("High.High", "High.Low", "Low.High", "Low.Low", "Not significant"),
                                     labels=c("High  - High", "High - Low", "Low - High","Low - Low", "Not significant"))

  ### PLOT

  cases <- ggplot() +
    geom_sf(data=ds2, aes(fill=patterns2), color="NA") +
    geom_sf(data = states, fill = "NA") +
    scale_fill_manual(values = c("red", "pink", "light blue", "grey80")) +
    guides(fill = guide_legend(title="Patterns")) + theme(
      legend.position = "bottom"
    )

#------------------#
#--LISA for Deaths--#
#------------------#

ds2$localI_death <- localmoran(as.numeric(ds2$DEATH100), lw)[,4]
ds2$localI_death_sigbreaks <- findInterval(ds2$localI_death, breaks, all.inside = TRUE)

# Identify high and low clusters using the interaction between the weights matrix and the standardized cases
deaths_z <- scale(ds2$DEATH100)[,1]
  patterns <- as.character( interaction(deaths_z > 0, W%*%deaths_z > 0) )
  patterns <- patterns %>%
      str_replace_all("TRUE","High") %>%
      str_replace_all("FALSE","Low")

  patterns[ds2$localI_death_sigbreaks == 4] <- "Not significant"
  ds2$patterns <- patterns

  # Rename LISA clusters
  ds2$patterns2 <- factor(ds2$patterns,
                                     levels=c("High.High", "High.Low", "Low.High", "Low.Low", "Not significant"),
                                     labels=c("High  - High", "High - Low", "Low - High","Low - Low", "Not significant"))

  ### PLOT

  deaths <- ggplot() +
    geom_sf(data=ds2, aes(fill=patterns2), color="NA") +
    geom_sf(data = states, fill = "NA") +
    scale_fill_manual(values = c("red", "pink", "light blue", "grey80")) +
    guides(fill = guide_legend(title="Patterns")) + theme(
      legend.position = "bottom"
    )

```

Below is an attempt to reproduce the multiple comparisons corrections in the supplementary figures using bonferroni/fdr corrections. The bonferroni adjustment uses the "p.adjust" function from the stats package. The fdr adjustment uses the "fdrtool" function from the fdrtools package. The algorithm implemented in the function is described in detail in the documentation on page 5: https://cran.r-project.org/web/packages/fdrtool/fdrtool.pdf on page 5. There are many possible packages for performing fdr corrections: http://www.strimmerlab.org/notes/fdr.html. For univariate corrections, I tried just this one.

Analysis: The corrections, especially fdr, do not match the publication supp. figures at all. I do not know why. Theoretically, the bonferroni correction should be close. However, it is not. I do not know why it is not close.
```{r local}
#-------------------------------------------------------------------------#
#--Multiple comparison corrections for Cases/Deaths univariate Moran's I--#
#-------------------------------------------------------------------------#

# function takes the variable, dataframe, weight matrices, and multiple comparisons correction (as a string)
univariate_mc <- function(variable, df, WM1, WM2, mc_approach) {
  # quosures https://stackoverflow.com/questions/43438001/how-to-pass-column-names-into-a-function-dplyr
  variable <- enquo(variable)
  var1 <- quo_name(variable)

  var_vec <- df %>%
    dplyr::pull(var1)
  
  var_vec
  
  # Moran's I values
  df$localI_case <- localmoran(as.numeric(var_vec), WM1)[,4]
  
  
  # P-values
  cases_p_values <- localmoran(as.numeric(var_vec), WM1)[,5]


  if (mc_approach == "bonferroni") {
    # bonferroni adjustment
    cases_p <- p.adjust(cases_p_values, method = "bonferroni")
    cases_sig <- (cases_p < 0.05)
    df$cases_sig <- cases_sig

  } else {
    # FDR adjustment
    cases_p <- fdrtool(cases_p_values, statistic = "pvalue", plot = FALSE)
    cases_p <- unlist(cases_p[1])
    cases_sig <- (cases_p < 0.05)
    df$cases_sig <- cases_sig

  }

  # Identify high and low clusters using the interaction between the weights matrix and the standardized cases
  cases_z <- scale(df$CASS100)[,1]
  patterns <- as.character( interaction(cases_z > 0, WM2%*%cases_z > 0) )
  patterns <- patterns %>%
      str_replace_all("TRUE","High") %>%
      str_replace_all("FALSE","Low")

  # bonferroni adjustment
  patterns[df$cases_sig == FALSE] <- "Not significant"
  df$patterns <- patterns

  # Rename LISA clusters
  df$patterns2 <- factor(df$patterns,
                                     levels=c("High.High", "High.Low", "Low.High", "Low.Low", "Not significant"),
                                     labels=c("High  - High", "High - Low", "Low - High","Low - Low", "Not significant"))

  ### PLOT

  ### PLOT
  colors <- c("High  - High" = "red", "High - Low" = "pink", "Low - High" = "light blue", "Low - Low" = "dark blue", "Not significant" = "grey80")

  theme_set(theme_void())

  g <- ggplot() +
    geom_sf(data=df, aes(fill=patterns2), color="NA") +
    geom_sf(data = states, fill = "NA") +
    scale_fill_manual(values = colors) +
    guides(fill = guide_legend(title="Patterns")) + ggtitle(paste("  ", mc_approach, " ", var1)) + theme(
      legend.position = "bottom"
    )

  return(g)
}


```



```{r}
cases_bonferroni <- univariate_mc("CASS100", ds2, lw, W, "bonferroni")
cases_fdr <- univariate_mc("CASS100", ds2, lw, W, "fdr")
deaths_bonferroni <- univariate_mc("DEATH100", ds2, lw, W, "bonferroni")
deaths_fdr <- univariate_mc("DEATH100", ds2, lw, W, "fdr")
```

```{r bivariate functions}
#------------------------#
#-- bivariate Moran's I--#
#------------------------#

#-- Takes vectors of predictors/responses to be tested with Bivariate Moran's I --#
variables_func <- function(predictors, responses, df, df_states, WM, mc_approach) {
  ggplots <- list()
  count = 1
  for (i in predictors) {
    for (j in responses) {
      ggplots[[count]] <- individual_tests(i,j,df, df_states, WM, mc_approach)
      count = count + 1
    }
  }
  return(ggplots)
}

#-- Performs individual Bivariate Moran's I tests for x,y pairs --#
individual_tests <- function(predictor, response, df, df_states, WM, mc_approach) {

  x <- df %>%
    dplyr::pull(predictor)

  y <- df %>%
    dplyr::pull(response)
 
  #-- PERFORM BIVARIATE MORAN'S I --#
  m <- moran_I(x, y, WM)

  #global bivariate
  (global_moran <- m[[1]][1])

  # Local values
  m_i <- m[[2]] # calculated Moran's I value for each county.
  
  # local simulations
  local_sims <- simula_moran(x, y, WM)$local_sims # returns matrix. 999 simulations (columns) for each county (rows).
  
  # global pseudo p-value
  global_sims <- simula_moran(x, y, WM)$global_sims

  # Proportion of simulated global values that are higher (in absolute terms) than the actual index
  moran_pvalue <- sum(abs(global_sims) > abs( global_moran )) / length(global_sims)
    
    
  # Logic to deal with no correction, bonferroni, or fdr
  if (mc_approach == "none") {
    # Identifying the significant values
    alpha <- .05  # for a 95% confidence interval; bonferroni adjustment would be dividing this value by 3108.
    probs <- c(alpha/2, 1-alpha/2)
    intervals <- t( apply(local_sims, 1, function(x) quantile(x, probs=probs))) # t() transposes matrix; apply(X, margin, function) applies function to margins of matrix;
    sig <- ( m_i < intervals[,1] )  | ( m_i > intervals[,2] ) # Is the Moran's I value outside the intervals (significant); row number = total # of counties.
  }  else {

    n = nrow(WM)
    p_vec <- numeric(n) # allocate p-value vector
    
    # calculate local pseudo p-value
    for (i in 1:n) {
      p_vec[[i]] <- sum(abs(local_sims[i,]) > abs(m_i[i])) / length(local_sims[i,])
    }

    if (mc_approach == "bonferroni") {
      # Stats package
      # adj_p_vec <- p.adjust(p_vec, method = "bonferroni")
      # sig <- (adj_p_vec < 0.05)
      
      # Custom correction (alpha divided by number of comparisons (3105))
      alpha <- 0.000016103  # for a 95% confidence interval; bonferroni adjustment would be dividing this value by 3108.
      probs <- c(alpha/2, 1-alpha/2)
      intervals <- t( apply(local_sims, 1, function(x) quantile(x, probs=probs))) # t() transposes matrix; apply(X, margin, function) applies function to margins of matrix;
      sig <- ( m_i < intervals[,1] )  | ( m_i > intervals[,2] ) # Is the Moran's I value outside the intervals (significant); row number = total # of counties.
      
    }
    else {
      # Stats package
      # adj_p_vec <- p.adjust(p_vec, method = "fdr")
      
      # fdrtools package
      adj_p_vec <- fdrtool(p_vec, statistic = "pvalue", plot = FALSE)
      adj_p_vec <- unlist(adj_p_vec[1])
      sig <- (adj_p_vec < 0.05)
    }
  }

  df$sig <- sig

  # Plotting
  g <- vis_tests(x, y, predictor, response, df, df_states, WM, mc_approach)
}

#-- LISA visualizations --#
vis_tests <- function(x, y, predictor, response, df, df_states, WM, mc_approach) {
  
  # Identifying the LISA clusters
    xp <- scale(x)[,1]
    yp <- scale(y)[,1]

  patterns <- as.character( interaction(xp > 0, WM%*%yp > 0))
  
  patterns <- patterns %>%
      str_replace_all("TRUE","High") %>%
      str_replace_all("FALSE","Low")
  
  patterns[df$sig==0] <- "Not significant" # not sure what this is doing.
  # print(patterns)
  df$patterns <- patterns

  # Rename LISA clusters
  df$patterns2 <- factor(df$patterns, levels=c("High.High", "High.Low", "Low.High", "Low.Low", "Not significant"),
                                    labels=c("High  - High", "High - Low", "Low - High","Low - Low", "Not significant"))
  
  ### PLOT
  colors <- c("High  - High" = "red", "High - Low" = "pink", "Low - High" = "light blue", "Low - Low" = "dark blue", "Not significant" = "grey80")
  
  if (mc_approach == "none") {
    g <- ggplot() +
      geom_sf(data=df, aes(fill=patterns2), color="NA") +
      geom_sf(data = df_states, fill = "NA") +
      scale_fill_manual(values = colors) +
      guides(fill = guide_legend(title = "Patterns")) +
      theme_minimal() + theme(
        legend.position = "bottom"
      )
  } else {
    
    g <- ggplot() +
      geom_sf(data=df, aes(fill=patterns2), color="NA") +
      geom_sf(data = df_states, fill = "NA") +
      scale_fill_manual(values = colors) +
      guides(fill = guide_legend(title = "Patterns")) +
      theme_minimal() + ggtitle(paste(mc_approach, " ", predictor, " vs ", response)) + theme(
        legend.position = "bottom"
      )
    
  }
  
  return(g)
  
  # ggsave(path = output_maps, paste0(predictor,response,".png"), height = 4, width = 6, scale = 1.5)
  
#   print(g)
# <<<<<<< HEAD
# =======
#   ggsave(path = output_maps, paste0(predictor,response,".png"), height = 4, width = 6, scale = 1.5)
# >>>>>>> c2524d5dc49b0f4089759e45813808e6d27e7ef2
}


# CODE BORROWED FROM https://gist.github.com/rafapereirabr/5348193abf779625f5e8c5090776a228
#-- Bivariate Moran's I --#
moran_I <- function(x, y = NULL, W){
  if(is.null(y)) y = x
  
  xp <- scale(x)[, 1]
  yp <- scale(y)[, 1]
  W[which(is.na(W))] <- 0
  n <- nrow(W)
  
  global <- (xp%*%W%*%yp)/(n - 1)
  local  <- (xp*W%*%yp)
  
  list(global = global, local  = as.numeric(local))
}

#-- Permutations for the Bivariate Moran's I --#
simula_moran <- function(x, y = NULL, W, nsims = 999){
  
  if(is.null(y)) y = x
  
  n   = nrow(W)
  IDs = 1:n
  
  xp <- scale(x)[, 1]
  W[which(is.na(W))] <- 0
  
  global_sims = NULL
  local_sims  = matrix(NA, nrow = n, ncol=nsims)
  
  ID_sample = sample(IDs, size = n*nsims, replace = T)
  
  y_s = y[ID_sample]
  y_s = matrix(y_s, nrow = n, ncol = nsims)
  y_s <- (y_s - apply(y_s, 1, mean))/apply(y_s, 1, sd)
  
  global_sims  <- as.numeric( (xp%*%W%*%y_s)/(n - 1) )
  local_sims  <- (xp*W%*%y_s)
  
  list(global_sims = global_sims,
       local_sims  = local_sims)
}
```


```{r}
predictors <- c("ICUBEDS", "OBESITY", "DIABETS", "BLACK", "HISPANC", "WHITE", "UNINSRD", "VACCNTN")
responses <- c("CASS100", "DEATH100")
```


```{r bivariate, results = T}
#--------------------------------------------#

ggp <- variables_func(responses, predictors, ds2, states, W, "none") #--- Per the figure foot notes, Saffary looked at responses by predictor as opposed to predictor by response
ggp_PCP_filter <- variables_func(responses, ("PCP"), ds3, states, W2, "none")
ggp_PCP_impute_zero <- variables_func(responses, ("PCP_impute_zero"), ds2, states, W, "none")
ggp_PCP_impute_mean <- variables_func(responses, ("PCP_impute_mean"), ds2, states, W, "none")
```

Psuedo P-values were calculated for each county using the same formula as the global Moran's I p-value: Proportion of simulated values (in absolute terms) that are higher than the calculated Moran's I value. I am pretty confident that my method for calculating the pseudo p-values for the supp. figures is correct because I am using the same method that reproduced the global Moran's I p-values in table 2. Although I am calculating pseudo p-values for local rather than global, the method should work for both because I am still comparing calculated Moran's I values against the 999 simulated values.


BONFERRONI CORRECTION
I tried two methods to perform the bonferroni correction. First, I divided the alpha value by the number of comparisons (3105), keeping all other code the same as when we did non-corrected maps (which matched the paper well). Strangely, these maps did not match the supplementary bon. figures very well. Next, I used the "p.adjust" function from the stats package to perform the bonferroni correction. With this method, I calculated psuedo p-values (See note above). This package produced very similar (if not identical?) to the manually dividing the alpha method. This provides evidence that the psuedo p-value calculation is correct.

FDR CORRECTION

I tried two different packages (out of many possible http://www.strimmerlab.org/notes/fdr.html) to calculate the false discover rate. First, I used the "p.adjust" function from the stats package https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/p.adjust. The documentation says their fdr algorithm is patterned after this paper: 

Benjamini, Y., and Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society Series B, 57, 289--300. http://www.jstor.org/stable/2346101.

Next, I used the "fdrtool" function from the fdrtool package. The algorithm implemented in the function is described in detail in the documentation on page 5: https://cran.r-project.org/web/packages/fdrtool/fdrtool.pdf on page 5

Analysis: 
Bonferroni: The bonferroni corrections I produced did not match the Saffary et al. maps. The Saffary bonferroni corrections have almost no significant counties. The reproductions have some significant counties. Both approaches produced the same map reproductions. I do not know how to explain the differences between our reproductions and Saffary supp. figures. The bonferroni correction should not have variability between packages, approaches. This idea was supported by my two approaches that produced the same maps.

FDR: Neither method reproduced the fdrmaps-adjusted maps perfectly. The p.adjust seemed to create better-matching maps for the ICUBEDS variable, but the fdrtools method seemed to create better-matching maps for all other variables. Overall, the variation between the sets of maps I produced was much less than the variation between my maps and the maps from the paper. Not sure what that means. Not sure where the differences are coming from.
```{r bivariate supplementary, results = T}
ggp_supp_fdr <- variables_func(responses, predictors, ds2, states, W, "fdr")
ggp_supp_bon <- variables_func(responses, predictors, ds2, states, W, "bonferroni")

ggp_supp_fdr_PCP <- variables_func(responses, ("PCP"), ds3, states, W2, "fdr")
ggp_supp_bon_PCP <- variables_func(responses, ("PCP"), ds3, states, W2, "bonferroni")
```



```{r figures}
# placeholder ggplots
void1 <- ggplot() + theme_void() 
void2 <- ggplot() + theme_void()
void3 <- ggplot() + theme_void()

#--------------------------------#
#---- Figure PCP_impute_zero ----#
#--------------------------------#

ggp_PCP_impute_zero[[1]]
ggsave(path = output_maps, "Cases_PCP_impute_zero.png", height = 4, width = 6, scale = 2.5)

ggp_PCP_impute_zero[[2]]
ggsave(path = output_maps, "Deaths_PCP_impute_zero.png", height = 4, width = 6, scale = 2.5)

#--------------------------------#
#---- Figure PCP_impute_mean ----#
#--------------------------------#

ggp_PCP_impute_mean[[1]]
ggsave(path = output_maps, "Cases_PCP_impute_mean.png", height = 4, width = 6, scale = 2.5)
ggp_PCP_impute_mean[[2]]
ggsave(path = output_maps, "Deaths_PCP_impute_mean.png", height = 4, width = 6, scale = 2.5)

#------------------#
#---- Figure 1 ----#
#------------------#

ggarrange(void1, void2, cases, deaths,
                    labels = c("A", "B", "C", "D"), heights = c(0.5,5),
                    ncol = 2, nrow = 2)

ggsave(path = output_maps, "fig1.png", height = 4, width = 6, scale = 2.5)

#-------------------------------#
#-- Figure 2 (PCP filter NA) ---#
#-------------------------------#

ggarrange(void1, void2,  ggp[[1]], ggp_PCP_filter[[1]], ggp[[9]], ggp_PCP_filter[[2]],
                    labels = c("A", "D", "B", "E", "C", "F"), heights = c(1,5,5),
                    ncol = 2, nrow = 3)

ggsave(path = output_maps, "fig2_PCP_filtered.png", height = 4, width = 6, scale = 2.5)

#---------------------------------#
#-- Figure 2 (PCP impute zero) ---#
#---------------------------------#

ggarrange(void1, void2,  ggp[[1]], ggp_PCP_impute_zero[[1]], ggp[[9]], ggp_PCP_impute_zero[[2]],
                    labels = c("A", "D", "B", "E", "C", "F"), heights = c(1,5,5),
                    ncol = 2, nrow = 3)

ggsave(path = output_maps, "fig2_PCP_impute_zero.png", height = 4, width = 6, scale = 2.5)

#---------------------------------#
#-- Figure 2 (PCP impute mean) ---#
#---------------------------------#

ggarrange(void1, void2,  ggp[[1]], ggp_PCP_impute_mean[[1]], ggp[[9]], ggp_PCP_impute_mean[[2]],
                    labels = c("A", "D", "B", "E", "C", "F"), heights = c(1,5,5),
                    ncol = 2, nrow = 3)

ggsave(path = output_maps, "fig2_PCP_impute_mean.png", height = 4, width = 6, scale = 2.5)


#------------------#
#---- Figure 3 ----#
#------------------#

ggarrange(void1, void2, ggp[[3]], ggp[[11]],
                    labels = c("A", "B", "C", "D"), heights = c(0.5,5),
                    ncol = 2, nrow = 2)

ggsave(path = output_maps, "fig3.png", height = 4, width = 6, scale = 2.5)


# ------------------#
# ---- Figure 5 ----#
# ------------------#
ggarrange(void1, void2, void3, ggp[[4]], ggp[[5]], ggp[[6]], ggp[[12]], ggp[[13]], ggp[[14]], font.label = list(size = 23),
                    labels = c("A", "D", "G", "B", "E", "H", "C", "F", "I"), heights = c(1,5,5),
                    ncol = 3, nrow = 3)

ggsave(path = output_maps, "fig5.png", height = 4, width = 6, scale = 3.8)

```


```{r supp. figures}
# univariate
ggarrange(cases_bonferroni, cases_fdr, deaths_bonferroni, deaths_fdr, ncol = 2, nrow = 2)
ggsave(path = output_maps, "supp_univariate.png", height = 5, width = 5, scale = 2.0)

# bivariate 1st 12
void1 <- ggplot() + theme_void()
ggarrange(ggp_supp_bon[[1]], ggp_supp_fdr[[1]], ggp_supp_bon[[9]], ggp_supp_fdr[[9]], ggp_supp_bon_PCP[[1]], ggp_supp_fdr_PCP[[1]], ggp_supp_bon_PCP[[2]], ggp_supp_fdr_PCP[[2]], ggp_supp_bon[[3]], ggp_supp_fdr[[3]], ggp_supp_bon[[11]], ggp_supp_fdr[[11]], ncol = 2, nrow = 6)

ggsave(path = output_maps, "supp_bivariate_1.png", height = 10, width = 3, scale = 4.0)

# bivariate 2nd 12
void1 <- ggplot() + theme_void()
ggarrange(ggp_supp_bon[[4]], ggp_supp_fdr[[4]], ggp_supp_bon[[11]], ggp_supp_fdr[[11]], ggp_supp_bon[[5]], ggp_supp_fdr[[5]], ggp_supp_bon[[13]], ggp_supp_fdr[[13]], ggp_supp_bon[[6]], ggp_supp_fdr[[6]], ggp_supp_bon[[14]], ggp_supp_fdr[[14]], ncol = 2, nrow = 6)

ggsave(path = output_maps, "supp_bivariate_2.png", height = 10, width = 3, scale = 4.0)
```




<<<<<<< HEAD
ggplot() +
  geom_sf(data=ds2, aes(fill=patterns2), color="NA") +
  scale_fill_manual(values = c("red", "pink", "light blue", "dark blue", "grey80")) + 
  guides(fill = guide_legend(title="LISA clusters")) +
  theme_minimal()
```
=======

>>>>>>> c2524d5dc49b0f4089759e45813808e6d27e7ef2
