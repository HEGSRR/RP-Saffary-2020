---
title: "Reproduction"
author: "Sarah Bardin and Josh Gilman"
date: "3/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, results='hide', warning = F, message = F)

rm(list=ls())

library(tidyverse)
library(spdep)
library(maptools)
library(sp)
library(rgdal)
library(DCluster)
library(readr)
library(readxl)
library(tidycensus)
library(sf)
library(here)
library(spatialreg)

output_maps <- here("results", "maps")

```


```{r}
rm(list=ls())
```



# Data Preparation
## Create Working File
In order to reproduce Saffary et al.'s paper *Spatial Dynamics of COVID-19 in US Counties*, we need to first create a working data set. To do this, we've downloaded the data provided by Saffary et al. (data_22.05.2020.xlsx) as well as a Census county-level boundary file. We merge the two files together and restrict to counties with non-missing data. This achieves a sample size of 3,142 counties, which is the same sample size reported by Saffery et al. for their analysis.

```{r read_in}
##-----------------------##
#------ READ IN DATA -----#
##-----------------------##
#---County Health Rankings Data---#
chr.pcp <- read_xlsx((here("data", "raw", "public", "2020 County Health Rankings Data - v2.xlsx")), sheet = "Ranked Measure Data", range =("A2:DF3195"))

chr.pcp <- chr.pcp %>%
              transmute(GEOID = FIPS,
                        PCP = `Primary Care Physicians Rate`)
head(chr.pcp)

#---Saffery Data---#
orig.data <- read_xlsx(here("data", "raw", "public", "data_22.05.2020.xlsx"))

#-Clean up variable names-#
names(orig.data) <- substr(names(orig.data),1,regexpr(",",names(orig.data)))
names(orig.data) <- substr(names(orig.data),1,nchar(names(orig.data))-1)
str(orig.data)

#---Boundary Data---#
boundaries <- get_estimates(geography = "county", 
                     product = "population", 
                     year = 2018,                  ## Saffary used 2018 boundary data
                     geometry = TRUE)

boundaries <- boundaries %>% 
                  filter(variable == "POP") 

str(boundaries)
str(orig.data)
str(chr.pcp)
##-----------------------##
#------ MERGE DATA  -----#
##-----------------------##
int.data <- left_join(orig.data, chr.pcp, by = "GEOID")
int.data <- int.data %>%
                dplyr::select(GEOID,
                              ICUBEDS, 
                              PCP,
                              DIABETS, 
                              OBESITY,
                              BLACK, 
                              HISPANC, 
                              WHITE, 
                              UNINSRD, 
                              VACCNTN,
                              CASS100, 
                              DEATH100,
                              CASES,
                              DEATHS,
                              STATENM)
str(int.data)
ds <- inner_join(boundaries, int.data, by = "GEOID") 
ds <- st_transform(ds, 5070)
ds <- ds %>% filter(!is.na(STATENM))  ## Results in 3,142 obs which matches article

summary(ds$PCP)

```

## Reproduce Global Moran's I 
After creating a working data file, we attempt to reproduce the global Moran's I statistics for the cases and deaths outcomes. We must use a the zero.policy = TRUE option in the weights construction, as there are counties with no neighbors in the data set. In part, this is due to the fact that the data set with 3,142 counties includes counties in Hawaii and Alaska. Because Saffery et al. indicated that Hawaii and Alaska were omitted from the analysis, we drop these counties, resulting in a data file with 3,108 counties. We again attempt to reproduce the global Moran's I statistics and again we find that there are counties without neighbors; an investigation into the noncontiguous counties identifed that Nantucket, MA is an island county. Despite needing to allow for counties with no neighbors, we are able to achieve similar I statisitics for both outcomes (to the third decimal place). **NOTE:** With rounding, we'd get an I statistic that is 0.001 higher than reported in the publication.

```{r global, results= T}
##------------------------------------##
#------ PERFORM GLOBAL MORAN'S I  -----#
##------------------------------------##
# Create spatial weights matrix with queen adjacency and binary connectivity
QN <- poly2nb(ds, queen = TRUE)
QN1.lw <- nb2listw(QN, style = "B", zero.policy = TRUE) ## there are empty neighbor sets
                                                        ## so used zero.policy option
moran.test(as.numeric(ds$DEATH100), QN1.lw, zero.policy = TRUE)

##------------------------------------##
#----- TEST OUT DROPPING AK AND HI ----#
##------------------------------------##
ds2 <- ds %>% filter(substr(GEOID,1,2) != "02" & substr(GEOID,1,2) != "15") 
ds2 <- ds2[-c(329,1168,1816),] ##-- EVEN IF WE REMOVE THE 3 COUNTIES WITH NO NEIGHBORS I STATISTIC DOESN'T CHANGE

#--Define Weighting Scheme--#
nb <- poly2nb(ds2, queen = TRUE)
lw <- nb2listw(nb, style = "B", zero.policy = TRUE) ## there are still empty neighbor sets
                                                              ## so used zero.policy option
W  <- as(lw, "symmetricMatrix")
W  <- as.matrix(W/rowSums(W))
W[which(is.na(W))] <- 0

#--DEATH RATE--#
moran.test(as.numeric(ds2$DEATH100), lw, zero.policy = TRUE)

#--CASE RATE--#
moran.test(as.numeric(ds2$CASS100), lw, zero.policy = TRUE)

```

After reproducing the global moran's I for the case and death rates, respectively, we also attempted to reproduce the local univariate moran's I for the case and death rates. We produced maps to mimic those in the original publication to assist in our visual inspection of the results. There are some discrepancies between these maps and those in Figures 1.c and 1.d. of the publication.


```{r local, results= T}
#Create Local Moran's I Plot
theme_set(theme_void())

# set breaks to critical z values
breaks <- c(-100, -2.58, -1.96, -1.65, 1.65, 1.96, 2.58, 100)

#------------------#
#--LISA for Cases--#
#------------------#

ds2$localI_case <- localmoran(as.numeric(ds2$CASS100), lw)[,4]

# findInterval() assigns ranks to the zvalues based on which bin the z values would fall into where the bins are broken up by the "breaks" variable created above
ds2$localI_case_sigbreaks <- findInterval(ds2$localI_case, breaks, all.inside = TRUE)

# Identify high and low clusters using the interaction between the weights matrix and the standardized cases
cases_z <- scale(ds2$CASS100)[,1]
  patterns <- as.character( interaction(cases_z > 0, W%*%cases_z > 0) )
  patterns <- patterns %>%
      str_replace_all("TRUE","High") %>%
      str_replace_all("FALSE","Low")
  
  patterns[ds2$localI_case_sigbreaks == 4] <- "Not significant"
  ds2$patterns <- patterns

  # Rename LISA clusters
  ds2$patterns2 <- factor(ds2$patterns, 
                                     levels=c("High.High", "High.Low", "Low.High", "Low.Low", "Not significant"),
                                     labels=c("High  - High", "High - Low", "Low - High","Low - Low", "Not significant"))
  
  ### PLOT

  cases <- ggplot() +
    geom_sf(data=ds2, aes(fill=patterns2), color="NA") +
    scale_fill_manual(values = c("red", "pink", "light blue", "grey80")) +
    guides(fill = guide_legend(title="LISA clusters")) + ggtitle("Cases LISA")

  ggsave(path = output_maps, "fig1_cases_lisa.png", height = 4, width = 6, scale = 1.5)
#------------------#
#--LISA for Deaths--#
#------------------#
  
ds2$localI_death <- localmoran(as.numeric(ds2$DEATH100), lw)[,4]
ds2$localI_death_sigbreaks <- findInterval(ds2$localI_death, breaks, all.inside = TRUE)

# Identify high and low clusters using the interaction between the weights matrix and the standardized cases
deaths_z <- scale(ds2$DEATH100)[,1]
  patterns <- as.character( interaction(deaths_z > 0, W%*%deaths_z > 0) )
  patterns <- patterns %>%
      str_replace_all("TRUE","High") %>%
      str_replace_all("FALSE","Low")

  patterns[ds2$localI_death_sigbreaks == 4] <- "Not significant"
  ds2$patterns <- patterns

  # Rename LISA clusters
  ds2$patterns2 <- factor(ds2$patterns, 
                                     levels=c("High.High", "High.Low", "Low.High", "Low.Low", "Not significant"),
                                     labels=c("High  - High", "High - Low", "Low - High","Low - Low", "Not significant"))
  
  ### PLOT

  deaths <- ggplot() +
    geom_sf(data=ds2, aes(fill=patterns2), color="NA") +
    scale_fill_manual(values = c("red", "pink", "light blue", "grey80")) +
    guides(fill = guide_legend(title="LISA clusters")) + ggtitle("Deaths LISA") 

  ggsave(path = output_maps, "fig1_deaths_lisa.png", height = 4, width = 6, scale = 1.5)
  

```


```{r bivariate functions}
#-- Takes vectors of predictors/responses to be tested with Bivariate Moran's I --#
variables_func <- function(predictors, responses, df, WM) {
  for (i in predictors) {
    for (j in responses) {
      individual_tests(i,j,df,WM)
    }
  }
}

#-- Performs individual Bivariate Moran's I tests for x,y pairs --#
# quosures if needed https://stackoverflow.com/questions/43438001/how-to-pass-column-names-into-a-function-dplyr
individual_tests <- function(predictor, response, df, WM) {

  x <- df %>%
    dplyr::pull(predictor)

  y <- df %>%
    dplyr::pull(response)
 
  #-- PERFORM BIVARIATE MORAN'S I --#
  m <- moran_I(x, y, WM)

  #global bivariate
  (global_moran <- m[[1]][1])

  # Local values
  m_i <- m[[2]]

  # local simulations
    local_sims <- simula_moran(x, y, WM)$local_sims

  # global pseudo p-value
  # get all simulated global moran
    global_sims <- simula_moran(x, y, WM)$global_sims

  # Proportion of simulated global values that are higher (in absolute terms) than the actual index
    moran_pvalue <- sum(abs(global_sims) > abs( global_moran )) / length(global_sims)
    #> 0

  # Identifying the significant values
    alpha <- .05  # for a 95% confidence interval
    probs <- c(alpha/2, 1-alpha/2)
    intervals <- t( apply(local_sims, 1, function(x) quantile(x, probs=probs)))
    sig <- ( m_i < intervals[,1] )  | ( m_i > intervals[,2] )
    df$sig <- sig
    
    # Plotting
    vis_tests(x, y, predictor, response, df, WM)
}

#-- LISA visualizations --#
vis_tests <- function(x, y, predictor, response, df, WM) {
  
  # Identifying the LISA clusters
    xp <- scale(x)[,1]
    yp <- scale(y)[,1]

  patterns <- as.character( interaction(xp > 0, WM%*%yp > 0) )
  patterns <- patterns %>%
      str_replace_all("TRUE","High") %>%
      str_replace_all("FALSE","Low")

  patterns[df$sig==0] <- "Not significant"
  df$patterns <- patterns

  # Rename LISA clusters
  df$patterns2 <- factor(df$patterns, levels=c("High.High", "High.Low", "Low.High", "Low.Low", "Not significant"),
                                    labels=c("High  - High", "High - Low", "Low - High","Low - Low", "Not significant"))
  
  ### PLOT

  g <- ggplot() +
    geom_sf(data=df, aes(fill=patterns2), color="NA") +
    scale_fill_manual(values = c("red", "pink", "light blue", "dark blue", "grey80")) +
    guides(fill = guide_legend(title="LISA clusters")) + ggtitle(paste("LISA ", response, " vs ", predictor)) +
    theme_minimal()
  
  print(g)
<<<<<<< HEAD
=======
  ggsave(path = output_maps, paste0(predictor,response,".png"), height = 4, width = 6, scale = 1.5)
>>>>>>> c2524d5dc49b0f4089759e45813808e6d27e7ef2
}


# CODE BORROWED FROM https://gist.github.com/rafapereirabr/5348193abf779625f5e8c5090776a228
#-- Bivariate Moran's I --#
moran_I <- function(x, y = NULL, W){
  if(is.null(y)) y = x
  
  xp <- scale(x)[, 1]
  yp <- scale(y)[, 1]
  W[which(is.na(W))] <- 0
  n <- nrow(W)
  
  global <- (xp%*%W%*%yp)/(n - 1)
  local  <- (xp*W%*%yp)
  
  list(global = global, local  = as.numeric(local))
}

#-- Permutations for the Bivariate Moran's I --#
simula_moran <- function(x, y = NULL, W, nsims = 999){
  
  if(is.null(y)) y = x
  
  n   = nrow(W)
  IDs = 1:n
  
  xp <- scale(x)[, 1]
  W[which(is.na(W))] <- 0
  
  global_sims = NULL
  local_sims  = matrix(NA, nrow = n, ncol=nsims)
  
  ID_sample = sample(IDs, size = n*nsims, replace = T)
  
  y_s = y[ID_sample]
  y_s = matrix(y_s, nrow = n, ncol = nsims)
  y_s <- (y_s - apply(y_s, 1, mean))/apply(y_s, 1, sd)
  
  global_sims  <- as.numeric( (xp%*%W%*%y_s)/(n - 1) )
  local_sims  <- (xp*W%*%y_s)
  
  list(global_sims = global_sims,
       local_sims  = local_sims)
}
```


```{r bivariate, results = T}
#--------------------------------------------#
predictors = c("ICUBEDS")
#, "DIABETS", "OBESITY","BLACK", "HISPANC", "WHITE", "UNINSRD", "VACCNTN") # all predictors
responses = c("CASS100", "DEATH100")

variables_func(responses, predictors, ds2, W) #--- Per the figure foot notes, Saffary looked at responses by predictor as opposed to predictor by response

```


Progress Notes:
1.) Need to figure out how to impute PCP variable.

2.) I did not write the code to save the figure outputs to the Rproject. This will probably be a good thing to do because it takes a while to run the code chunk that does all Moran calculations

<<<<<<< HEAD
ggplot() +
  geom_sf(data=ds2, aes(fill=patterns2), color="NA") +
  scale_fill_manual(values = c("red", "pink", "light blue", "dark blue", "grey80")) + 
  guides(fill = guide_legend(title="LISA clusters")) +
  theme_minimal()
```
=======

>>>>>>> c2524d5dc49b0f4089759e45813808e6d27e7ef2
